{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d2c5345-b9ef-4f8c-80b5-c450da975a86",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "getting raw data"
    }
   },
   "source": [
    "1. Upload files to s3 bucket folder and create a volume in databricks mounting on the same location\n",
    "    - To create a external volume\n",
    "    `CREATE EXTERNAL VOLUME if not exists bronze.datavault.ccm LOCATION 'S3 path'`\n",
    "2. Create a managed volume and upload the files using databricks UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e59648-3895-43b4-b5e5-357f1cc70b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "dbutils.widgets.text(\"bronze_catalog\", \"bronze_sb\")\n",
    "dbutils.widgets.text(\"bronze_schema\", \"test3\")\n",
    "catalog_name = dbutils.widgets.get(\"bronze_catalog\")\n",
    "schema_name = dbutils.widgets.get(\"bronze_schema\")\n",
    "raw_volume_path = \"/Volumes/bronze_sb/test3/raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13771334-3611-4fcf-b1ee-a8be985fec34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# volume has been created to store the raw files\n",
    "spark.sql(f\"create volume if not exists {catalog_name}.{schema_name}.raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3690434-1eb5-4383-828f-7f04e06d9483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/fact.averagecosts.dlm', name='fact.averagecosts.dlm', size=29142959, modificationTime=1741510042000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/fact.transactions.dlm', name='fact.transactions.dlm', size=380631122, modificationTime=1741510141000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.clnd.dlm', name='hier.clnd.dlm', size=276119, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.hldy.dlm', name='hier.hldy.dlm', size=545, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.invloc.dlm', name='hier.invloc.dlm', size=2702, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.invstatus.dlm', name='hier.invstatus.dlm', size=17004, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.possite.dlm', name='hier.possite.dlm', size=5079, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.pricestate.dlm', name='hier.pricestate.dlm', size=157, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.prod.dlm', name='hier.prod.dlm', size=194621, modificationTime=1741510035000), FileInfo(path='dbfs:/Volumes/bronze_sb/test3/raw_data/hier.rtlloc.dlm', name='hier.rtlloc.dlm', size=3456, modificationTime=1741510035000)]\n"
     ]
    }
   ],
   "source": [
    "# getting the metadata of the files\n",
    "dataset_list = dbutils.fs.ls(raw_volume_path)\n",
    "print(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e548c1f2-14ad-4df8-8b03-0690fef91cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Volumes/bronze_sb/test3/raw_data/fact.averagecosts.dlm', '/Volumes/bronze_sb/test3/raw_data/fact.transactions.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.clnd.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.hldy.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.invloc.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.invstatus.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.possite.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.pricestate.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.prod.dlm', '/Volumes/bronze_sb/test3/raw_data/hier.rtlloc.dlm']\n"
     ]
    }
   ],
   "source": [
    "# extracting the filepaths from the metadata collected\n",
    "file_paths = [x.path[5:] for x in dataset_list]\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a38da3cf-04e4-4c67-bd0e-f9fc9a8c033e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data files have been uploaded using UI due to available resources but the following piece of code can be used to directly read from S3 cloud and proceed further.\n",
    "1. Install the Hadoop-aws library on the compute\n",
    "2. Set the AWS credentials in the Spark configuration\n",
    "3. Read the files by providing full s3 paths\n",
    "'''\n",
    "\n",
    "# Set AWS credentials in Spark configuration\n",
    "# spark.conf.set(\"spark.hadoop.fs.s3a.access.key\", \"YOUR_ACCESS_KEY\")\n",
    "# spark.conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"YOUR_SECRET_KEY\")\n",
    "# spark.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "# spark.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "# Now we can use df_from_file_path to load the data but s3 full path needs to be passed in this case like \"s3a://raw_data_bucket/folders/file.dlm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3903c91-0db8-4657-bef9-adeef4a82dc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "funtions"
    }
   },
   "outputs": [],
   "source": [
    "# function to load the raw data into lakehouse schema(bronze_sb) with the given dataframe\n",
    "def df_load(target_catalog,schema_name,table_name,df,mode=\"overwrite\"):\n",
    "    # to avoid the table name containing any special characters\n",
    "    table_name = table_name.replace(\"-\",\"_\").replace('.',\"_\")\n",
    "    table = f\"{target_catalog}.{schema_name}.{table_name}\"\n",
    "    # to create the managed table in the lakehouse with overwrite mode and blocksize of 64mb\n",
    "    df.write.format(\"delta\").mode(mode).option(\"overwriteSchema\", \"true\").option(\"maxRecordsPerFile\", 1000000).option(\"parquet.block.size\", 64 * 1024 * 1024).saveAsTable(table)\n",
    "    row_count = df.count()\n",
    "    print(f\"Table: {table} has been loaded with {row_count} rows\")\n",
    "\n",
    "# function to form the dataframe from the filepaths\n",
    "def df_from_filepath(file_path, file_name):\n",
    "    # to read the delimited file with inferring schema and header as true\n",
    "    df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"|\").option(\"inferSchema\", \"true\").csv(file_path)\n",
    "    # calling the function to create the table in the staging schema\n",
    "    df_load(catalog_name, schema_name, file_name, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b5d44e-8f0c-4c02-8288-f391cef10173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingesting fact.averagecosts.dlm\n",
      "Table: bronze_sb.test3.fact_averagecosts_dlm has been loaded with 740805 rows\n",
      "ingesting fact.transactions.dlm\n",
      "Table: bronze_sb.test3.fact_transactions_dlm has been loaded with 4503108 rows\n",
      "ingesting hier.clnd.dlm\n",
      "Table: bronze_sb.test3.hier_clnd_dlm has been loaded with 1820 rows\n",
      "ingesting hier.hldy.dlm\n",
      "Table: bronze_sb.test3.hier_hldy_dlm has been loaded with 19 rows\n",
      "ingesting hier.invloc.dlm\n",
      "Table: bronze_sb.test3.hier_invloc_dlm has been loaded with 85 rows\n",
      "ingesting hier.invstatus.dlm\n",
      "Table: bronze_sb.test3.hier_invstatus_dlm has been loaded with 245 rows\n",
      "ingesting hier.possite.dlm\n",
      "Table: bronze_sb.test3.hier_possite_dlm has been loaded with 90 rows\n",
      "ingesting hier.pricestate.dlm\n",
      "Table: bronze_sb.test3.hier_pricestate_dlm has been loaded with 4 rows\n",
      "ingesting hier.prod.dlm\n",
      "Table: bronze_sb.test3.hier_prod_dlm has been loaded with 1235 rows\n",
      "ingesting hier.rtlloc.dlm\n",
      "Table: bronze_sb.test3.hier_rtlloc_dlm has been loaded with 84 rows\n"
     ]
    }
   ],
   "source": [
    "# Loading data with b_ prefix indicating bronze representing raw data\n",
    "for file in file_paths:\n",
    "    file_name = file.split(\"/\")[-1]\n",
    "    print(f\"ingesting {file_name}\")\n",
    "    df_from_filepath(file, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7035dfbd-e038-490e-8aa4-ab7b4118bc6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "checks"
    }
   },
   "outputs": [],
   "source": [
    "def primary_key_check(table_name, primary_keys):\n",
    "    non_null = False\n",
    "    uniqueness = False\n",
    "    nulls = 0\n",
    "    for pk in primary_keys:\n",
    "        nulls += spark.sql(f\"select count(*) from {table_name} where {pk} is null\").collect()[0][0]\n",
    "    if nulls == 0:\n",
    "        non_null = True\n",
    "    keys = ','.join(primary_keys)\n",
    "    uniqueness = spark.sql(f\"select count(*) from {table_name} group by {keys} having count(*) > 1\").collect()\n",
    "    if len(uniqueness) == 0:\n",
    "        uniqueness = True\n",
    "    return non_null and uniqueness\n",
    "\n",
    "def foreign_key_check(table_name1, table_name2, foreign_key):\n",
    "    is_foreign = False\n",
    "    fails = spark.sql(f\"select count(*) from {table_name1} where {foreign_key} not in (select {foreign_key} from {table_name2})\").collect()[0][0]\n",
    "    if fails == 0:\n",
    "        is_foreign = True\n",
    "        print(f\"Foreign keys {foreign_key} for {table_name1} and {table_name2} are satisfied\")\n",
    "    else:\n",
    "        print(f\"Foreign keys {foreign_key} for {table_name1} and {table_name2} are not satisfied\")\n",
    "    return is_foreign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02560172-0520-4fca-bf29-756cadcbee21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking primary keys for hier_clnd_dlm\n",
      "Primary keys fscldt_id are satisfied for hier_clnd_dlm\n",
      "Checking primary keys for hier_hldy_dlm\n",
      "Primary keys hldy_id are satisfied for hier_hldy_dlm\n",
      "Checking primary keys for hier_invloc_dlm\n",
      "Primary keys loc are satisfied for hier_invloc_dlm\n",
      "Checking primary keys for hier_invstatus_dlm\n",
      "Primary keys code_id are satisfied for hier_invstatus_dlm\n",
      "Checking primary keys for hier_possite_dlm\n",
      "Primary keys site_id are satisfied for hier_possite_dlm\n",
      "Checking primary keys for hier_pricestate_dlm\n",
      "Primary keys state_id, substate_id are satisfied for hier_pricestate_dlm\n",
      "Checking primary keys for hier_prod_dlm\n",
      "Primary keys sku_id are satisfied for hier_prod_dlm\n",
      "Checking primary keys for hier_rtlloc_dlm\n",
      "Primary keys str are satisfied for hier_rtlloc_dlm\n",
      "Checking primary keys for fact_averagecosts_dlm\n",
      "Primary keys fscldt_id, sku_id are satisfied for fact_averagecosts_dlm\n"
     ]
    }
   ],
   "source": [
    "# Primary keys are formed based on ERD in step 1\n",
    "primary_keys_dict = {\"hier_clnd_dlm\" : \"fscldt_id\" , \"hier_hldy_dlm\" : \"hldy_id\" , \"hier_invloc_dlm\" : \"loc\" , \"hier_invstatus_dlm\" : \"code_id\" , \"hier_possite_dlm\" : \"site_id\" , \"hier_pricestate_dlm\" : \"state_id, substate_id\" , \"hier_prod_dlm\" : \"sku_id\" , \"hier_rtlloc_dlm\" : \"str\", \"fact_averagecosts_dlm\" : \"fscldt_id, sku_id\"}\n",
    "for table_name, primary_keys in primary_keys_dict.items():\n",
    "    print(f\"Checking primary keys for {table_name}\")\n",
    "    is_primary = primary_key_check(f\"{catalog_name}.{schema_name}.{table_name}\", primary_keys.split(\",\"))\n",
    "    if not is_primary:\n",
    "        print(\"**************\")\n",
    "        print(f\"Primary keys {primary_keys} are not satisfied for {table_name}\")\n",
    "        print(\"**************\")\n",
    "    else:\n",
    "        print(f\"Primary keys {primary_keys} are satisfied for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449f3cbc-e7b8-432f-ae35-e430fa43a88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreign keys fscldt_id for bronze_sb.test3.fact_averagecosts_dlm and bronze_sb.test3.hier_clnd_dlm are not satisfied\n",
      "False\n",
      "Foreign keys sku_id for bronze_sb.test3.fact_averagecosts_dlm and bronze_sb.test3.hier_prod_dlm are satisfied\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# # Foreign key check for b_fact_averagecosts_dlm against b_hier_clnd_dlm and b_hier_prod_dlm\n",
    "# print(foreign_key_check(f\"{catalog_name}.{schema_name}.fact_averagecosts_dlm\", f\"{catalog_name}.{schema_name}.hier_clnd_dlm\", \"fscldt_id\"))\n",
    "# print(foreign_key_check(f\"{catalog_name}.{schema_name}.fact_averagecosts_dlm\", f\"{catalog_name}.{schema_name}.hier_prod_dlm\", \"sku_id\"))\n",
    "# # Hence sku_id is the foreign key and fscldt_id is not but it is part of primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "404d1420-7ad8-4932-963b-5cc94e82dfbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "scratch_help"
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# select * from silver_sb.information_schema.tables where table_name like 'b%'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3504500991255134,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2a.bronze_load_checks",
   "widgets": {
    "bronze_catalog": {
     "currentValue": "bronze_sb",
     "nuid": "d03907fb-effe-4e28-8a91-71af92243c79",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze_sb",
      "label": null,
      "name": "bronze_catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze_sb",
      "label": null,
      "name": "bronze_catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "bronze_schema": {
     "currentValue": "test3",
     "nuid": "337edd37-880a-4cb5-94be-3139355cc6d7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "test3",
      "label": null,
      "name": "bronze_schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "test3",
      "label": null,
      "name": "bronze_schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
